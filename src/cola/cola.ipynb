{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "cola.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johsieders/fttai/blob/main/src/cola/cola.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlZU_dgFeFqI"
      },
      "source": [
        "# from https://mccormickml.com/2019/07/22/BERT-fine-tuning\n",
        "# The Corpus of Linguistic Acceptability (CoLA)\n",
        "\n",
        "# Johannes Siedersleben\n",
        "# QAware GmbH, Munich\n",
        "# 10.2.2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK-TPo6WGop4"
      },
      "source": [
        "# uncomment this if import fails\r\n",
        "# !pip install wget\r\n",
        "\r\n",
        "import os\r\n",
        "import zipfile\r\n",
        "import wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipASXUiMGwhs"
      },
      "source": [
        "# download and unzip raw data\r\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\r\n",
        "\r\n",
        "zipped_file = 'cola_public_1.1.zip'\r\n",
        "zipped_dir = './cola_public_1.1/'\r\n",
        "unzipped_file = './cola_public/raw/in_domain_train.tsv'\r\n",
        "\r\n",
        "if not os.path.exists(zipped_file):\r\n",
        "    wget.download(url, zipped_file)\r\n",
        "print('download successful')\r\n",
        "\r\n",
        "if not os.path.exists(zipped_dir):\r\n",
        "    zip = zipfile.ZipFile(zipped_file)\r\n",
        "    zip.extractall()\r\n",
        "\r\n",
        "print('unzipped file now at ' + unzipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ICHpPxGSeFqQ"
      },
      "source": [
        "# uncomment this if import fails\n",
        "# !pip install transformers\n",
        "\n",
        "# Python imports\n",
        "import random\n",
        "import pickle\n",
        "from collections.abc import Callable\n",
        "from time import perf_counter\n",
        "\n",
        "# utilities for download and file import\n",
        "import pandas as pd\n",
        "\n",
        "# neural metworks support: torch, Huggingface transformers\n",
        "import torch\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "W_5FlpTheFqR"
      },
      "source": [
        "class Logger(object):\n",
        "    def __init__(self):\n",
        "        self.protocol = []\n",
        "        self.counter = 0\n",
        "        self.char_counter = 0\n",
        "\n",
        "    def log(self, input: any) -> None:\n",
        "        print(self.counter, end='')  # I am working\n",
        "        self.counter = (self.counter + 1) % 10\n",
        "        self.char_counter =(self.char_counter + 1) % 80\n",
        "        if self.char_counter == 0:\n",
        "          print()\n",
        "        self.protocol.append((perf_counter(), input))\n",
        "\n",
        "    def getProtocol(self) -> list:\n",
        "        return self.protocol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "y3moTqhGeFqR"
      },
      "source": [
        "class Learner(object):\n",
        "    def __init__(self, module: torch.nn.Module,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 schedulerFactory: Callable,\n",
        "                 device: torch.device):\n",
        "        self.module = module\n",
        "        self.optimizer = optimizer\n",
        "        self.schedulerFactory = schedulerFactory\n",
        "        self.scheduler = None\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, dataloader: DataLoader, logger: Logger) -> None:\n",
        "        \"\"\"\n",
        "        @param dataloader: a dataloader with input_ids at 0, attention_mask at 1, labels at 2\n",
        "        @param logger: a logger\n",
        "        @return: None\n",
        "        This is one epoch. the essential loop of artificial intelligence.\n",
        "        It runs over all training sentences, one minibatch at a time.\n",
        "        One loop takes about 2 seconds on a i7 cpu!\n",
        "        \"\"\"\n",
        "        self.module.train()\n",
        "\n",
        "        for batch in dataloader:\n",
        "            loss = self.module.forward(input_ids=batch[0],\n",
        "                                       token_type_ids=None,\n",
        "                                       attention_mask=batch[1],\n",
        "                                       labels=batch[2]).loss\n",
        "            logger.log(loss.item())\n",
        "            loss.backward()  # compute gradient\n",
        "            clip_grad_norm_(module.parameters(), 1.0)  # normalize gradient\n",
        "            self.optimizer.step()  # do one optimization step\n",
        "            self.scheduler.step()  # adjust learning rate\n",
        "            self.optimizer.zero_grad()  # reset gradient\n",
        "\n",
        "    def predict(self, dataloader: DataLoader) -> tuple:\n",
        "        \"\"\"\n",
        "        @param dataloader: a dataloader with input_ids at 0, attention_mask at 1, labels at 2\n",
        "        @return: tuple of (label, prediction), two tensors\n",
        "        \"\"\"\n",
        "        self.module.eval()\n",
        "        labels = torch.tensor((), dtype=torch.int, device=self.device)\n",
        "        predictions = torch.tensor((), dtype=torch.int, device=self.device)\n",
        "\n",
        "        for batch in dataloader:\n",
        "            with torch.no_grad():\n",
        "                logits = self.module.forward(input_ids=batch[0],\n",
        "                                             token_type_ids=None,\n",
        "                                             attention_mask=batch[1],\n",
        "                                             labels=batch[2]).logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            labels = torch.cat((labels, batch[2]))    # collect labels\n",
        "            predictions = torch.cat((predictions, preds))  # collect predictions\n",
        "\n",
        "        return labels, predictions\n",
        "\n",
        "    def fit(self, dataloader: DataLoader, steps_per_epoch, n_epochs: int) -> list:\n",
        "        \"\"\"\n",
        "        @param dataloader: a dataloader with input_ids at 0, attention_mask at 1, labels at 2\n",
        "        @param steps_per_epoch: number of steps per epoch\n",
        "        @param n_epochs: number of epochs\n",
        "        @return: the protocol\n",
        "        \"\"\"\n",
        "        total_steps = n_epochs * steps_per_epoch\n",
        "        self.scheduler = self.schedulerFactory(total_steps)\n",
        "        logger = Logger()\n",
        "        for i in range(n_epochs):\n",
        "            logger.log(f'epoch {i}')\n",
        "            self.train(dataloader, logger)\n",
        "        return logger.getProtocol()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bapd_McOeFqS"
      },
      "source": [
        "def getDevice(cuda_desired: bool) -> torch.device:\n",
        "    \"\"\"\n",
        "    @param cuda_desired: True if cuda desired\n",
        "    @return: cuda if desired and available, cpu otherwise\n",
        "    \"\"\"\n",
        "    return torch.device('cuda') if cuda_desired and torch.cuda.is_available() \\\n",
        "        else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Jvh0S1SdeFqT"
      },
      "source": [
        "def readSentencesLabels(filename: str,\n",
        "                       n_sentences: int,\n",
        "                       col_sentence: int,\n",
        "                       col_label: int,\n",
        "                       delimiter: str = '\\t') -> tuple:\n",
        "    \"\"\"\n",
        "    @param filename: file to be read from\n",
        "    @param delimiter: a delimiter\n",
        "    @param col_sentence: index of column of sentences\n",
        "    @param col_label: index of column of labels\n",
        "    @return: a tuple containing a list of sentences and a list of labels\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(filename, delimiter=delimiter, nrows=n_sentences, header=None)\n",
        "    return df[col_sentence].values.tolist(), df[col_label].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ACJwmh42eFqT"
      },
      "source": [
        "def tokenize(sentences: list,\n",
        "           tokenizer: BertTokenizer,\n",
        "           max_length: int) -> tuple:\n",
        "    \"\"\"\n",
        "    @param sentences: list of sentences\n",
        "    @param tokenizer: a tokenizer\n",
        "    @param max_length: sentences to be padded to\n",
        "    @return: list of token_ids, list of attention_masks\n",
        "    Encoding proceeds as follows:\n",
        "    (1) Tokenize the sentence.\n",
        "    (2) Prepend the `[CLS]` token to the start.\n",
        "    (3) Append the `[SEP]` token to the end.\n",
        "    (4) Map tokens to their IDs.\n",
        "    (5) Pad or truncate the sentence to `max_length`\n",
        "    (6) Create attention mask for [PAD] tokens.\n",
        "\n",
        "    Note: pad_to_max_length is deprecated, no way to get around.\n",
        "    \"\"\"\n",
        "    token_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for s in sentences:\n",
        "        encoded_dict = tokenizer(s,\n",
        "                                 add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "                                 truncation=True,\n",
        "                                 max_length=max_length,\n",
        "                                 pad_to_max_length=True,\n",
        "                                 return_attention_mask=True\n",
        "                                 )\n",
        "        token_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "        # third entry (token_type_ids) not needed\n",
        "\n",
        "    return token_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "H-JqoZDmeFqU"
      },
      "source": [
        "def getDataloader(token_ids: list,\n",
        "                  attention_masks: list,\n",
        "                  labels: list,\n",
        "                  split_factor: float,\n",
        "                  batch_size: int,\n",
        "                  device: torch.device) -> tuple:\n",
        "    \"\"\"\n",
        "    @param token_ids: token_ids, plain Python list, (n_sentences x max_length)\n",
        "    @param attention_masks: attention_masks, plain Python list (n_sentences x max_length)\n",
        "    @param labels: labels, plain Python list (len = n_sentences)\n",
        "    @param split_factor: share of training sentences\n",
        "    @param batch_size: size of minibatch\n",
        "    @param device: device the dataloaders are on\n",
        "    @return: tuple of two dataloaders, one for training and one for test\n",
        "\n",
        "    Dataloaders return on each call a list of k 3-tupels (token_ids, attention_mask, label);\n",
        "    with k = batch_size. All returned elements are torch.tensors on the requested device\n",
        "    \"\"\"\n",
        "\n",
        "    token_ids = torch.tensor(token_ids, device=device)\n",
        "    attention_masks = torch.tensor(attention_masks, device=device)\n",
        "    labels = torch.tensor(labels, device=device)\n",
        "\n",
        "    dataset = TensorDataset(token_ids, attention_masks, labels)\n",
        "    train_size = int(split_factor * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        sampler=RandomSampler(train_dataset),\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=SequentialSampler(test_dataset),\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LmiFZsspeFqV"
      },
      "source": [
        "def getModule(device: torch.device) -> torch.nn.Module:\n",
        "    module = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",  # Use the 12-layer BERT module, with an uncased vocab.\n",
        "        num_labels=2,  # The number of output labels--2 for binary classification.\n",
        "        output_attentions=False,  # do not return attentions weights.\n",
        "        output_hidden_states=False,  # do not return hidden-states.\n",
        "    )\n",
        "    if device == torch.device('cpu'):\n",
        "        module.cpu()\n",
        "    else:\n",
        "        module.cuda()\n",
        "    return module"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JflN_RSleFqV"
      },
      "source": [
        "def getTokenizer() -> BertTokenizer:\n",
        "    \"\"\"\n",
        "    @return: the Bert Tokenizer\n",
        "    \"\"\"\n",
        "    return BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rGDXxbmieFqW"
      },
      "source": [
        "def getOptimizer(module: torch.nn.Module,\n",
        "                 lr: float,\n",
        "                 eps: float) -> torch.optim.Optimizer:\n",
        "    \"\"\"\n",
        "    @param module: a module\n",
        "    @param lr: learning rate\n",
        "    @param eps: stop criterion\n",
        "    @return: the Adam optimizer (any other choice is valid)\n",
        "    \"\"\"\n",
        "    return AdamW(module.parameters(), lr=lr, eps=eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xC7fYp5HeFqW"
      },
      "source": [
        "def getSchedulerFactory(optimizer: torch.optim.Optimizer) -> Callable:\n",
        "    \"\"\"\n",
        "    @param optimizer: an optimizer\n",
        "    @return: a function which returns a scheduler depending on the total number of optimizer steps.\n",
        "    \"\"\"\n",
        "\n",
        "    def factory(total_steps: int):\n",
        "        return get_linear_schedule_with_warmup(optimizer,\n",
        "                                               num_warmup_steps=0,\n",
        "                                               num_training_steps=total_steps)\n",
        "    return factory\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lzf4pwWjeFqX"
      },
      "source": [
        "# put constants in a dictionary\n",
        "cfg = {'seed': 2,\n",
        "       'batch_size': 32,\n",
        "       'n_sentences': 1000,  # number of sentences to read\n",
        "       'max_length': 64,      # max length of sentence (guess or find out)\n",
        "       'split_factor': 0.8,   # share of training sentences\n",
        "       'cuda_desired': True,  # True if cuda desired\n",
        "       'lr': 3e-5,            # learning rate of optimizer\n",
        "       'eps': 1e-8,           # stop criterion of optimizer\n",
        "       'n_epochs' : None}     # number of epochs\n",
        "\n",
        "seed = cfg['seed']\n",
        "batch_size = cfg['batch_size']\n",
        "n_sentences = cfg['n_sentences']\n",
        "max_length = cfg['max_length']\n",
        "split_factor = cfg['split_factor']\n",
        "cuda_desired = cfg['cuda_desired']\n",
        "lr = cfg['lr']\n",
        "eps = cfg['eps']\n",
        "\n",
        "# seed randomizers\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cOkbGvb7eFqX"
      },
      "source": [
        "# extract at most n sentences and labels; n = n_sentences\n",
        "unzipped_file = './cola_public/raw/in_domain_train.tsv'\n",
        "col_sentence = 3  # index of column of sentences\n",
        "col_label = 1  # index of column of labels\n",
        "\n",
        "sentences, labels = readSentencesLabels(unzipped_file, n_sentences, col_sentence, col_label)\n",
        "\n",
        "n = len(sentences)\n",
        "k = len(list(filter(lambda x: x == 1, labels)))\n",
        "\n",
        "print(n, k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2A7Tt5aueFqZ"
      },
      "source": [
        "# define the algorithm\n",
        "device = getDevice(cuda_desired)  # device depending on choice and availability of cuda\n",
        "module = getModule(device)  # a BERT model or any other\n",
        "tokenizer = getTokenizer()\n",
        "optimizer = getOptimizer(module, lr, eps)\n",
        "schedulerFactory = getSchedulerFactory(optimizer)  # learner can be called with different values of n_epoch\n",
        "steps_per_epoch = int(len(sentences) * split_factor / batch_size) + 1\n",
        "\n",
        "# tokenize sentences to token ids and attention masks\n",
        "token_ids, attention_masks = tokenize(sentences, tokenizer, max_length)\n",
        "\n",
        "# put token ids, attention masks and labels into a dataloader\n",
        "train_dataloader, test_dataloader = \\\n",
        "    getDataloader(token_ids, attention_masks, labels, split_factor, batch_size, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "R4J4kec6eFqZ"
      },
      "source": [
        "# build a learner and get going\n",
        "cfg['n_epochs'] = 4\n",
        "learner = Learner(module, optimizer, schedulerFactory, device)\n",
        "protocol = learner.fit(train_dataloader, steps_per_epoch, cfg['n_epochs'])\n",
        "print('\\nfitting finished')\n",
        "\n",
        "# make predictions on train and test data\n",
        "train_labels, train_predictions = learner.predict(train_dataloader)\n",
        "test_labels, test_predictions = learner.predict(test_dataloader)\n",
        "log_object = (cfg,\n",
        "              protocol,\n",
        "              train_labels, train_predictions,\n",
        "              test_labels, test_predictions)\n",
        "\n",
        "# save protocol\n",
        "log_file = 'log_000.pickle'\n",
        "with open(log_file, 'wb') as log:\n",
        "    pickle.dump(log_object, log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcdjH4CTLwwQ"
      },
      "source": [
        "# How did we do?\r\n",
        "\r\n",
        "log_file = 'log_000.pickle'\r\n",
        "with open(log_file, 'rb') as log:\r\n",
        "    log_object = pickle.load(log)\r\n",
        "\r\n",
        "cfg, protocol, \\\r\n",
        "train_labels, train_predictions, \\\r\n",
        "test_labels, test_predictions = log_object\r\n",
        "\r\n",
        "# show the outcome\r\n",
        "print(f\"\\ntotal number of labels:         {len(train_labels)}\\n\"\r\n",
        "      f\"total number of correct labels: {len(list(filter(lambda x: x == 1, train_labels)))}\")\r\n",
        "\r\n",
        "starttime = protocol[0][0]\r\n",
        "print(f'\\n\\ntimestamp\\tinfo\\n')\r\n",
        "for timestamp, info in protocol[:]:\r\n",
        "    print(f'{timestamp - starttime:.6f}\\t{info}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}