{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "cola.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "UlZU_dgFeFqI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053440157,
     "user_tz": -60,
     "elapsed": 638,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "# from https://mccormickml.com/2019/07/22/BERT-fine-tuning\n",
    "# The Corpus of Linguistic Acceptability (CoLA)\n",
    "\n",
    "# Johannes Siedersleben\n",
    "# QAware GmbH, Munich\n",
    "# 10.2.2021"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICHpPxGSeFqQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443067,
     "user_tz": -60,
     "elapsed": 3543,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    },
    "outputId": "bef9a401-d7a3-4f2f-ede9-8485ac2279dc"
   },
   "source": [
    "# uncomment this if import fails\n",
    "!pip install transformers\n",
    "\n",
    "# Python imports\n",
    "import random\n",
    "import pickle\n",
    "from collections.abc import Callable\n",
    "from time import perf_counter\n",
    "\n",
    "# utilities for download and file import\n",
    "import pandas as pd\n",
    "\n",
    "# neural metworks support: torch, Huggingface transformers\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch import Tensor\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (4.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (4.56.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: packaging in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\j.siedersleben\\anaconda3\\envs\\ai\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "W_5FlpTheFqR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443067,
     "user_tz": -60,
     "elapsed": 3539,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.protocol = []\n",
    "        self.counter = 0\n",
    "\n",
    "    def log(self, input: any) -> None:\n",
    "        print(self.counter, end='')  # I am working\n",
    "        self.counter = (self.counter + 1) % 10\n",
    "        self.protocol.append((perf_counter(), input))\n",
    "\n",
    "    def getProtocol(self) -> list:\n",
    "        return self.protocol"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "y3moTqhGeFqR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443068,
     "user_tz": -60,
     "elapsed": 3537,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "class Learner(object):\n",
    "    def __init__(self, module: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 schedulerFactory: Callable,\n",
    "                 device: torch.device):\n",
    "        self.module = module\n",
    "        self.optimizer = optimizer\n",
    "        self.schedulerFactory = schedulerFactory\n",
    "        self.scheduler = None\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, dataloader: DataLoader, logger: Logger) -> None:\n",
    "        \"\"\"\n",
    "        @param dataloader: a dataloader with input_ids at 0, attention_mask at 1, labels at 2\n",
    "        @param logger: a logger\n",
    "        @return: None\n",
    "        This is one epoch. the essential loop of artificial intelligence.\n",
    "        It runs over all training sentences, one minibatch at a time.\n",
    "        One loop takes about 2 seconds on a i7 cpu!\n",
    "        \"\"\"\n",
    "        self.module.train()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            loss = self.module.forward(input_ids=batch[0],\n",
    "                                       token_type_ids=None,\n",
    "                                       attention_mask=batch[1],\n",
    "                                       labels=batch[2]).loss\n",
    "            logger.log(loss.item())\n",
    "            loss.backward()  # compute gradient\n",
    "            clip_grad_norm_(module.parameters(), 1.0)  # normalize gradient\n",
    "            self.optimizer.step()  # do one optimization step\n",
    "            self.scheduler.step()  # adjust learning rate\n",
    "            self.optimizer.zero_grad()  # reset gradient\n",
    "\n",
    "    def predict(self, dataloader: DataLoader) -> tuple:\n",
    "        \"\"\"\n",
    "        @param dataloader: a dataloader with input_ids at 0, attention_mask at 1, labels at 2\n",
    "        @return: tuple of (label, prediction), two tensors\n",
    "        \"\"\"\n",
    "        self.module.eval()\n",
    "        labels = torch.tensor((), dtype=torch.int, device=self.device)\n",
    "        predictions = torch.tensor((), dtype=torch.int, device=self.device)\n",
    "\n",
    "        for batch in dataloader:\n",
    "            with torch.no_grad():\n",
    "                logits = self.module.forward(input_ids=batch[0],\n",
    "                                             token_type_ids=None,\n",
    "                                             attention_mask=batch[1],\n",
    "                                             labels=batch[2]).logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            labels = torch.cat((labels, batch[2]))    # collect labels\n",
    "            predictions = torch.cat((predictions, preds))  # collect predictions\n",
    "\n",
    "        return labels, predictions\n",
    "\n",
    "    def fit(self, dataloader: DataLoader, steps_per_epoch, n_epochs: int) -> list:\n",
    "        \"\"\"\n",
    "        @param dataloader: a dataloader with input_ids at 0, attention_mask at 1, labels at 2\n",
    "        @param steps_per_epoch: number of steps per epoch\n",
    "        @param n_epochs: number of epochs\n",
    "        @return: the protocol\n",
    "        \"\"\"\n",
    "        total_steps = n_epochs * steps_per_epoch\n",
    "        self.scheduler = self.schedulerFactory(total_steps)\n",
    "        logger = Logger()\n",
    "        for i in range(n_epochs):\n",
    "            logger.log(f'epoch {i}')\n",
    "            self.train(dataloader, logger)\n",
    "        return logger.getProtocol()"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bapd_McOeFqS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443068,
     "user_tz": -60,
     "elapsed": 3535,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "def getDevice(cuda_desired: bool) -> torch.device:\n",
    "    \"\"\"\n",
    "    @param cuda_desired: True if cuda desired\n",
    "    @return: cuda if desired and available, cpu otherwise\n",
    "    \"\"\"\n",
    "    return torch.device('cuda') if cuda_desired and torch.cuda.is_available() \\\n",
    "        else torch.device('cpu')"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Jvh0S1SdeFqT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443069,
     "user_tz": -60,
     "elapsed": 3533,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "def readSentencesLabels(filename: str,\n",
    "                       n_sentences: int,\n",
    "                       col_sentence: int,\n",
    "                       col_label: int,\n",
    "                       delimiter: str = '\\t') -> tuple:\n",
    "    \"\"\"\n",
    "    @param filename: file to be read from\n",
    "    @param delimiter: a delimiter\n",
    "    @param col_sentence: index of column of sentences\n",
    "    @param col_label: index of column of labels\n",
    "    @return: a tuple containing a list of sentences and a list of labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, delimiter=delimiter, nrows=n_sentences, header=None)\n",
    "    return df[col_sentence].values.tolist(), df[col_label].values.tolist()"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ACJwmh42eFqT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443069,
     "user_tz": -60,
     "elapsed": 3529,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "def encode(sentences: list,\n",
    "           tokenizer: BertTokenizer,\n",
    "           max_length: int) -> tuple:\n",
    "    \"\"\"\n",
    "    @param sentences: list of sentences\n",
    "    @param tokenizer: a tokenizer\n",
    "    @param max_length: sentences to be padded to\n",
    "    @return: list of token_ids, list of attention_masks\n",
    "    Encoding proceeds as follows:\n",
    "    (1) Tokenize the sentence.\n",
    "    (2) Prepend the `[CLS]` token to the start.\n",
    "    (3) Append the `[SEP]` token to the end.\n",
    "    (4) Map tokens to their IDs.\n",
    "    (5) Pad or truncate the sentence to `max_length`\n",
    "    (6) Create attention mask for [PAD] tokens.\n",
    "\n",
    "    Note: pad_to_max_length is deprecated, no way to get around.\n",
    "    \"\"\"\n",
    "    token_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for s in sentences:\n",
    "        encoded_dict = tokenizer(s,\n",
    "                                 add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                                 truncation=True,\n",
    "                                 max_length=max_length,\n",
    "                                 pad_to_max_length=True,\n",
    "                                 return_attention_mask=True\n",
    "                                 )\n",
    "        token_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        # third entry (token_type_ids) not needed\n",
    "\n",
    "    return token_ids, attention_masks"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorize(token_ids: Tensor, attn_mask: Tensor, model: BertModel) -> Tensor:\n",
    "\n",
    "    output = model(token_ids, attn_mask)\n",
    "    hidden_states = output[2]\n",
    "    token_vecs = hidden_states[-2][0]  # shape = (#tokens, 768)\n",
    "    # result[i] = average token_vecs[i, j], j = 0 .. 767\n",
    "    vector = torch.mean(token_vecs, dim=0)  # shape = (768)\n",
    "    return vector\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "H-JqoZDmeFqU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443070,
     "user_tz": -60,
     "elapsed": 3527,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "def getDataloader(token_ids: list,\n",
    "                  attention_masks: list,\n",
    "                  labels: list,\n",
    "                  split_factor: float,\n",
    "                  batch_size: int,\n",
    "                  device: torch.device) -> tuple:\n",
    "    \"\"\"\n",
    "    @param token_ids: token_ids, plain Python list, (n_sentences x max_length)\n",
    "    @param attention_masks: attention_masks, plain Python list (n_sentences x max_length)\n",
    "    @param labels: labels, plain Python list (len = n_sentences)\n",
    "    @param split_factor: share of training sentences\n",
    "    @param batch_size: size of minibatch\n",
    "    @param device: device the dataloaders are on\n",
    "    @return: tuple of two dataloaders, one for training and one for test\n",
    "\n",
    "    Dataloaders return on each call a list of k 3-tupels (token_ids, attention_mask, label);\n",
    "    with k = batch_size. All returned elements are torch.tensors on the requested device\n",
    "    \"\"\"\n",
    "\n",
    "    token_ids = torch.tensor(token_ids, device=device)\n",
    "    attention_masks = torch.tensor(attention_masks, device=device)\n",
    "    labels = torch.tensor(labels, device=device)\n",
    "\n",
    "    dataset = TensorDataset(token_ids, attention_masks, labels)\n",
    "    train_size = int(split_factor * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler=SequentialSampler(test_dataset),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "LmiFZsspeFqV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443070,
     "user_tz": -60,
     "elapsed": 3524,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "def getModule(device: torch.device) -> torch.nn.Module:\n",
    "\n",
    "    module = torch.nn.Sequential(torch.nn.Linear(768, 1), torch.nn.Sigmoid())\n",
    "    if device == torch.device('cpu'):\n",
    "        module.cpu()\n",
    "    else:\n",
    "        module.cuda()\n",
    "    return module"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "JflN_RSleFqV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443070,
     "user_tz": -60,
     "elapsed": 3520,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "def getTokenizer() -> BertTokenizer:\n",
    "    \"\"\"\n",
    "    @return: the Bert Tokenizer\n",
    "    \"\"\"\n",
    "    return BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rGDXxbmieFqW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443071,
     "user_tz": -60,
     "elapsed": 3518,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "def getOptimizer(module: torch.nn.Module,\n",
    "                 lr: float,\n",
    "                 eps: float) -> torch.optim.Optimizer:\n",
    "    \"\"\"\n",
    "    @param module: a module\n",
    "    @param lr: learning rate\n",
    "    @param eps: stop criterion\n",
    "    @return: the Adam optimizer (any other choice is valid)\n",
    "    \"\"\"\n",
    "    return AdamW(module.parameters(), lr=lr, eps=eps)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xC7fYp5HeFqW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443294,
     "user_tz": -60,
     "elapsed": 3738,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "def getSchedulerFactory(optimizer: torch.optim.Optimizer) -> Callable:\n",
    "    \"\"\"\n",
    "    @param optimizer: an optimizer\n",
    "    @return: a function which returns a scheduler depending on the total number of optimizer steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def factory(total_steps: int):\n",
    "        return get_linear_schedule_with_warmup(optimizer,\n",
    "                                               num_warmup_steps=0,\n",
    "                                               num_training_steps=total_steps)\n",
    "    return factory\n"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lzf4pwWjeFqX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1613053443295,
     "user_tz": -60,
     "elapsed": 3737,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "# put constants in a dictionary\n",
    "cfg = {'seed': 2,\n",
    "       'batch_size': 8,\n",
    "       'n_sentences': 100,     # number of sentences to read\n",
    "       'max_length': 64,      # max length of sentence (guess or find out)\n",
    "       'split_factor': 0.8,   # share of training sentences\n",
    "       'cuda_desired': False, # True if cuda desired\n",
    "       'lr': 2e-5,            # learning rate of optimizer\n",
    "       'eps': 1e-8,           # stop criterion of optimizer\n",
    "       'n_epochs' : None}     # number of epochs\n",
    "\n",
    "seed = cfg['seed']\n",
    "batch_size = cfg['batch_size']\n",
    "n_sentences = cfg['n_sentences']\n",
    "max_length = cfg['max_length']\n",
    "split_factor = cfg['split_factor']\n",
    "cuda_desired = cfg['cuda_desired']\n",
    "lr = cfg['lr']\n",
    "eps = cfg['eps']\n",
    "\n",
    "# seed randomizers\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "cOkbGvb7eFqX",
    "executionInfo": {
     "status": "error",
     "timestamp": 1613053443298,
     "user_tz": -60,
     "elapsed": 3737,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    },
    "outputId": "913d1997-67a3-405e-e020-cb9745278e9c"
   },
   "source": [
    "# extract at most n sentences and labels; n = n_sentences\n",
    "unzipped_file = './cola_public/raw/in_domain_train.tsv'\n",
    "col_sentence = 3  # index of column of sentences\n",
    "col_label = 1  # index of column of labels\n",
    "\n",
    "sentences, labels = readSentencesLabels(unzipped_file, n_sentences, col_sentence, col_label)\n",
    "\n",
    "n = len(sentences)\n",
    "k = len(list(filter(lambda x: x == 1, labels)))\n",
    "\n",
    "print(n, k)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-8adb5c832f48>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mcol_label\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m  \u001B[0;31m# index of column of labels\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreadSentencesLabels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0munzipped_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_sentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol_sentence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-10-f7484605c452>\u001B[0m in \u001B[0;36mreadSentencesLabels\u001B[0;34m(filename, n_sentences, col_sentence, col_label, delimiter)\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0;32mreturn\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtuple\u001B[0m \u001B[0mcontaining\u001B[0m \u001B[0ma\u001B[0m \u001B[0mlist\u001B[0m \u001B[0mof\u001B[0m \u001B[0msentences\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0ma\u001B[0m \u001B[0mlist\u001B[0m \u001B[0mof\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \"\"\"\n\u001B[0;32m---> 13\u001B[0;31m     \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelimiter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdelimiter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mn_sentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol_sentence\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol_label\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[1;32m    686\u001B[0m     )\n\u001B[1;32m    687\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 688\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    689\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    690\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 454\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    455\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    946\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    947\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 948\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    949\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    950\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1178\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1179\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"c\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1180\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1181\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1182\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"python\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   2008\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"usecols\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2009\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2010\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2011\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2012\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './cola_public/raw/in_domain_train.tsv'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2A7Tt5aueFqZ",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1613053443296,
     "user_tz": -60,
     "elapsed": 3733,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "# define the algorithm\n",
    "device = getDevice(cuda_desired)  # device depending on choice and availability of cuda\n",
    "module = getModule(device)  # a BERT model or any other\n",
    "tokenizer = getTokenizer()\n",
    "optimizer = getOptimizer(module, lr, eps)\n",
    "schedulerFactory = getSchedulerFactory(optimizer)  # learner can be called with different values of n_epoch\n",
    "steps_per_epoch = int(len(sentences) * split_factor / batch_size) + 1\n",
    "\n",
    "# encode sentences to token ids and attention masks\n",
    "token_ids, attention_masks = encode(sentences, tokenizer, max_length)\n",
    "vector = vectorize(token_ids, attention_masks, module)\n",
    "\n",
    "# put token ids, attention masks and labels into a dataloader\n",
    "train_dataloader, test_dataloader = \\\n",
    "    getDataloader(token_ids, attention_masks, labels, split_factor, batch_size, device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "R4J4kec6eFqZ",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1613053443297,
     "user_tz": -60,
     "elapsed": 3732,
     "user": {
      "displayName": "Johannes Siedersleben",
      "photoUrl": "",
      "userId": "18160953325954532806"
     }
    }
   },
   "source": [
    "# build a learner and get going\n",
    "cfg['n_epochs'] = 4\n",
    "learner = Learner(module, optimizer, schedulerFactory, device)\n",
    "protocol = learner.fit(train_dataloader, steps_per_epoch, cfg['n_epochs'])\n",
    "print('\\nfitting finished')\n",
    "\n",
    "# make predictions on train and test data\n",
    "train_labels, train_predictions = learner.predict(train_dataloader)\n",
    "test_labels, test_predictions = learner.predict(test_dataloader)\n",
    "log_object = (cfg,\n",
    "              protocol,\n",
    "              train_labels, train_predictions,\n",
    "              test_labels, test_predictions)\n",
    "\n",
    "# save protocol\n",
    "log_file = 'log_000.pickle'\n",
    "with open(log_file, 'wb') as log:\n",
    "    pickle.dump(log_object, log)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}