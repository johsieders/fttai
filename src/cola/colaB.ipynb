{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johsieders/fttai/blob/main/src/cola/colaB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from https://mccormickml.com/2019/07/22/BERT-fine-tuning\n",
    "# The Corpus of Linguistic Acceptability (CoLA)\n",
    "\n",
    "# Johannes Siedersleben\n",
    "# QAware GmbH, Munich\n",
    "# 28.2.2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment this if import fails\n",
    "# !pip install wget\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download(url, zipped_file, zipped_dir, unzipped_file) -> None:\n",
    "    \"\"\"\n",
    "    download and unzip raw data\n",
    "    :param url: URL to be downloaded from\n",
    "    :param zipped_file: name of file to be downloaded\n",
    "    :param zipped_dir: target directory\n",
    "    :param unzipped_file: name of unzipped file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(zipped_file):\n",
    "        wget.download(url, zipped_file)\n",
    "    print('download successful')\n",
    "\n",
    "    if not os.path.exists(zipped_dir):\n",
    "        zip = zipfile.ZipFile(zipped_file)\n",
    "        zip.extractall()\n",
    "\n",
    "    print('unzipped file now at ' + unzipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# utilities for display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def showProgress(protocol: list) -> None:\n",
    "    \"\"\"\n",
    "    :param protocol: list of tuples (timestamp, loss)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    losses = [protocol[i][1] for i in range(len(protocol)) if type(protocol[i][1]) is float]\n",
    "    plt.figure()\n",
    "    plt.plot(losses, label='training loss')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations')\n",
    "\n",
    "\n",
    "def showMetrics(labels, predictions: Tensor) -> None:\n",
    "    \"\"\"\"\n",
    "    @param labels: tensor of labels\n",
    "    @param predictions: tensor of prediction\n",
    "    @return: None\n",
    "    This functions prints accuracy, precision, recall and f1\n",
    "    \"\"\"\n",
    "    labels = torch.tensor(labels, device=torch.device('cpu'))\n",
    "    predictions = torch.tensor(predictions, device=torch.device('cpu'))\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "\n",
    "    print(f'\\naccuracy  = {accuracy:.3f}\\n'\n",
    "          f'precision = {precision:.3f}\\n'\n",
    "          f'recall    = {recall:.3f}\\n'\n",
    "          f'f1        = {f1:.3f}')\n",
    "\n",
    "\n",
    "def showConfusionMatrix(labels, predictions: Tensor, names: list) -> None:\n",
    "    \"\"\"\n",
    "    @param labels: tensor of labels\n",
    "    @param predictions: tensor of prediction\n",
    "    @param names: names of categories, e.g. ['correct', 'incorrect']\n",
    "    @return: None\n",
    "    The confusion matrix is a K x K matrix with K = number of categories\n",
    "    \"\"\"\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    vmax = cm.max()   # number of categories\n",
    "    sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "                xticklabels=names, yticklabels=names, vmin=0, vmax=vmax, cmap=\"YlGnBu\")\n",
    "    plt.xlabel = ('true label')\n",
    "    plt.ylabel = ('predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment this if import fails\n",
    "# !pip install transformers\n",
    "\n",
    "# Python imports\n",
    "import random\n",
    "import pickle\n",
    "from collections.abc import Callable\n",
    "from time import perf_counter\n",
    "\n",
    "# utilities for download and file import\n",
    "import pandas as pd\n",
    "\n",
    "# neural metworks support: torch, Hugging Face transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.protocol = []\n",
    "        self.counter = 0\n",
    "        self.char_counter = 0\n",
    "\n",
    "    def log(self, input: any) -> None:\n",
    "        print(self.counter, end='')  # I am working\n",
    "        self.counter = (self.counter + 1) % 10\n",
    "        self.char_counter =(self.char_counter + 1) % 80\n",
    "        if self.char_counter == 0:\n",
    "          print()\n",
    "        self.protocol.append((perf_counter(), input))\n",
    "\n",
    "    def getProtocol(self) -> list:\n",
    "        return self.protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    def __init__(self, module: nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 loss_fct: Callable,\n",
    "                 device: torch.device):\n",
    "        self.module = module\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fct = loss_fct\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, dataloader: DataLoader, logger: Logger) -> None:\n",
    "        \"\"\"\n",
    "        @param dataloader: a dataloader with input_ids at 0, labels at 1\n",
    "        @param logger: a logger\n",
    "        @return: None\n",
    "        This is one epoch, the essential loop of artificial intelligence.\n",
    "        It runs over all training sentences, one minibatch at a time.\n",
    "        One loop takes about 2 seconds on a i7 cpu!\n",
    "        \"\"\"\n",
    "        self.module.train()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            vectors, labels = batch\n",
    "            labels = labels.view(-1, 1)\n",
    "            logits = self.module(vectors)\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "            logger.log(loss.item())\n",
    "            loss.backward()  # compute gradient\n",
    "            clip_grad_norm_(module.parameters(), 1.0)  # normalize gradient\n",
    "            self.optimizer.step()  # do one optimization step\n",
    "            self.optimizer.zero_grad()  # reset gradient\n",
    "\n",
    "    def predict(self, dataloader: DataLoader) -> tuple:\n",
    "        \"\"\"\n",
    "        @param dataloader: a dataloader with input_ids at 0, labels at 2\n",
    "        @return: tuple of (label, prediction), two tensors\n",
    "        \"\"\"\n",
    "        self.module.eval()\n",
    "        targets = torch.tensor((), dtype=torch.int, device=self.device)\n",
    "        predictions = torch.tensor((), dtype=torch.int, device=self.device)\n",
    "\n",
    "        for batch in dataloader:\n",
    "            vectors, labels = batch\n",
    "            with torch.no_grad():\n",
    "                logits = self.module(vectors)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            targets = torch.cat((targets, labels))    # collect targets\n",
    "            predictions = torch.cat((predictions, preds))  # collect predictions\n",
    "\n",
    "        return targets, predictions\n",
    "\n",
    "    def fit(self, dataloader: DataLoader, n_epochs: int) -> list:\n",
    "        \"\"\"\n",
    "        @param dataloader: a dataloader with input_ids at 0, attention_mask at 1, labels at 2\n",
    "        @param n_epochs: number of epochs\n",
    "        @return: the protocol\n",
    "        \"\"\"\n",
    "        logger = Logger()\n",
    "        for i in range(n_epochs):\n",
    "            logger.log(f'epoch {i}')\n",
    "            self.train(dataloader, logger)\n",
    "        return logger.getProtocol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getDevice(cuda_desired: bool) -> torch.device:\n",
    "    \"\"\"\n",
    "    @param cuda_desired: True if cuda desired\n",
    "    @return: cuda if desired and available, cpu otherwise\n",
    "    \"\"\"\n",
    "    return torch.device('cuda') if cuda_desired and torch.cuda.is_available() \\\n",
    "        else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def readData(filename: str,\n",
    "                        n_sentences: int,\n",
    "                        col_sentence: int,\n",
    "                        col_label: int,\n",
    "                        delimiter: str = '\\t') -> tuple:\n",
    "    \"\"\"\n",
    "    @param filename: file to be read from\n",
    "    @param n_sentences: number of sentences to be read\n",
    "    @param delimiter: a delimiter\n",
    "    @param col_sentence: index of column of sentences\n",
    "    @param col_label: index of column of labels\n",
    "    @return: a tuple containing a list of sentences and a list of labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, delimiter=delimiter, nrows=n_sentences, header=None)\n",
    "    return df[col_sentence].values.tolist(), df[col_label].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getTokenizer(device: torch.device) -> Callable:\n",
    "\n",
    "    bert_model = BertModel.from_pretrained(\n",
    "        \"bert-base-uncased\",  # Use the 12-layer BERT module, with an uncased vocab.\n",
    "        output_hidden_states=True,  # return hidden-states.\n",
    "    )\n",
    "    if device == torch.device('cpu'):\n",
    "        bert_model.cpu()\n",
    "    else:\n",
    "        bert_model.cuda()\n",
    "\n",
    "    bert_model.eval()\n",
    "\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    def tokenize(sentences: list, max_length: int) -> Tensor:\n",
    "        vectors = []\n",
    "\n",
    "        for s in sentences:\n",
    "            dict = bert_tokenizer(s,\n",
    "                             add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                             truncation=True,\n",
    "                             max_length=max_length,\n",
    "                             pad_to_max_length=True,\n",
    "                             return_attention_mask=True,\n",
    "                            )\n",
    "\n",
    "            toks = dict['input_ids']\n",
    "            attn = dict['attention_mask']\n",
    "            token_ids = torch.tensor(toks, device=device).view((1, -1))\n",
    "            attn_mask = torch.tensor(attn, device=device).view((1, -1))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = bert_model(token_ids, attn_mask)\n",
    "\n",
    "            hidden_states = output[2]\n",
    "            token_vectors = hidden_states[-2][0]  # shape = (#tokens, 768)\n",
    "            # result[i] = average token_vecs[i, j], j = 0 .. 767\n",
    "            vector = torch.mean(token_vectors, dim=0)  # shape = (768)\n",
    "            vectors.append(vector)\n",
    "\n",
    "        vectors = torch.cat(vectors).view(len(sentences), -1)\n",
    "        return vectors\n",
    "\n",
    "    return tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def getDataloader(token_ids: Tensor,\n",
    "                  labels: Tensor,\n",
    "                  split_factor: float,\n",
    "                  batch_size: int) -> tuple:\n",
    "    \"\"\"\n",
    "    @param token_ids: token_ids, plain Python list, (n_sentences x max_length)\n",
    "    @param labels: labels, plain Python list (len = n_sentences)\n",
    "    @param split_factor: share of training sentences\n",
    "    @param batch_size: size of minibatch\n",
    "    @return: tuple of two dataloaders, one for training and one for test\n",
    "\n",
    "    Dataloaders return on each call a list of k 3-tupels (token_ids, attention_mask, label);\n",
    "    with k = batch_size. All returned elements are torch.tensors on the requested device\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = TensorDataset(token_ids, labels)\n",
    "    train_size = int(split_factor * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler=SequentialSampler(test_dataset),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def getModule(device: torch.device) -> nn.Module:\n",
    "    module = nn.Linear(768, 1)\n",
    "    if device == torch.device('cpu'):\n",
    "        module.cpu()\n",
    "    else:\n",
    "        module.cuda()\n",
    "    return module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def getOptimizer(module: nn.Module,\n",
    "                 lr: float,\n",
    "                 eps: float) -> torch.optim.Optimizer:\n",
    "    \"\"\"\n",
    "    @param module: a module\n",
    "    @param lr: learning rate\n",
    "    @param eps: stop criterion\n",
    "    @return: the Adam optimizer (any other choice is valid)\n",
    "    \"\"\"\n",
    "    return AdamW(module.parameters(), lr=lr, eps=eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def getLossFunction() -> Callable:\n",
    "    \"\"\"\n",
    "    @return: binary cross entropy loss (or any other loss function)\n",
    "    \"\"\"\n",
    "    return nn.BCEWithLogitsLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the configuration and put it in a dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 8\n",
    "n_sentences = 1000\n",
    "max_length = 64\n",
    "split_factor = 0.8\n",
    "cuda_desired = True\n",
    "lr = 3e-5\n",
    "eps = 1e-8\n",
    "n_epochs = 4\n",
    "\n",
    "cfg = {'seed': seed,\n",
    "   'batch_size': batch_size,\n",
    "   'n_sentences': n_sentences,      # number of sentences to read\n",
    "   'max_length': max_length,        # max length of sentence (guess or find out)\n",
    "   'split_factor': split_factor,    # share of training sentences\n",
    "   'cuda_desired': cuda_desired,    # True if cuda desired\n",
    "   'lr': lr,                        # learning rate of optimizer\n",
    "   'eps': eps,                      # stop criterion of optimizer\n",
    "   'n_epochs': n_epochs}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set the seeds and determine device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = getDevice(cuda_desired)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download successful\n",
      "unzipped file now at ./cola_public/raw/in_domain_train.tsv\n"
     ]
    }
   ],
   "source": [
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "zipped_file = 'cola_public_1.1.zip'\n",
    "zipped_dir = './cola_public_1.1/'\n",
    "unzipped_file = './cola_public/raw/in_domain_train.tsv'\n",
    "download(url, zipped_file, zipped_dir, unzipped_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "col_sentence = 3  # index of column of sentences\n",
    "col_label = 1  # index of column of labels\n",
    "sentences, labels = readData(unzipped_file, n_sentences, col_sentence, col_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute token_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\j.siedersleben\\Anaconda3\\envs\\ai\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2137: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time for tokenizing: 24.8739\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = perf_counter()\n",
    "tokenizer = getTokenizer(device)\n",
    "token_ids = tokenizer(sentences, max_length)\n",
    "labels = torch.tensor(labels, dtype=torch.float32, device=device)\n",
    "\n",
    "t1 = perf_counter()\n",
    "print(f'elapsed time for tokenizing: {t1 - t0:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = getDataloader(token_ids, labels, split_factor, batch_size)\n",
    "module = getModule(device)     # Linear(768, 1)\n",
    "optimizer = getOptimizer(module, lr, eps)\n",
    "loss_fct = getLossFunction()\n",
    "learner = Learner(module, optimizer, loss_fct, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
      "01234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
      "01234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
      "01234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
      "01234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
      "0123\n",
      "elapsed time for training: 0.6249\n"
     ]
    }
   ],
   "source": [
    "t0 = perf_counter()\n",
    "protocol = learner.fit(train_dataloader, n_epochs)\n",
    "t1 = perf_counter()\n",
    "print(f'\\nelapsed time for training: {t1 - t0:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make predictions on train and test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train_labels, train_predictions = learner.predict(train_dataloader)\n",
    "test_labels, test_predictions = learner.predict(test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "log_object = (cfg,\n",
    "              protocol,\n",
    "              train_labels, train_predictions,\n",
    "              test_labels, test_predictions)\n",
    "\n",
    "log_file = 'log_000.pickle'\n",
    "with open(log_file, 'wb') as log:\n",
    "    pickle.dump(log_object, log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Show progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "elapsed time = 0.6238\n",
      "\n",
      "total number of train labels:          800\n",
      "total number of correct train labels:  513\n"
     ]
    }
   ],
   "source": [
    "log_file = 'log_000.pickle'\n",
    "with open(log_file, 'rb') as log:\n",
    "    log_object = pickle.load(log)\n",
    "\n",
    "cfg, protocol, \\\n",
    "train_labels, train_predictions, \\\n",
    "test_labels, test_predictions = log_object\n",
    "\n",
    "elapsed_time = protocol[-1][0] - protocol[0][0]\n",
    "print(f'\\nelapsed time = {elapsed_time:.4f}')\n",
    "\n",
    "# show the outcome\n",
    "print(f\"\\n{'total number of train labels:':38} {len(train_labels)}\\n\"\n",
    "      f\"{'total number of correct train labels:':38} {len(train_labels[train_labels == 1])}\")\n",
    "\n",
    "# showProgress(protocol)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Show metrics and confusion matrices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "result on train data\n",
      "\n",
      "accuracy  = 0.359\n",
      "precision = 0.000\n",
      "recall    = 0.000\n",
      "f1        = 0.000\n",
      "\n",
      "result on test data\n",
      "\n",
      "accuracy  = 0.335\n",
      "precision = 0.000\n",
      "recall    = 0.000\n",
      "f1        = 0.000\n",
      "\n",
      "result on test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-79d14805d3e5>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=torch.device('cpu'))\n",
      "<ipython-input-4-79d14805d3e5>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions = torch.tensor(predictions, device=torch.device('cpu'))\n",
      "C:\\Users\\j.siedersleben\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-25-80e78c3adf7f>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mcat_names\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'incorrect'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'correct'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'\\nresult on test data'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0mshowConfusionMatrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_labels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_predictions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcat_names\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'\\nresult on test data'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mshowConfusionMatrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_labels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_predictions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcat_names\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-4-79d14805d3e5>\u001B[0m in \u001B[0;36mshowConfusionMatrix\u001B[1;34m(labels, predictions, names)\u001B[0m\n\u001B[0;32m     49\u001B[0m     \"\"\"\n\u001B[0;32m     50\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m     \u001B[0mcm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfusion_matrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m     \u001B[0mvmax\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m   \u001B[1;31m# number of categories\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=True,\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     70\u001B[0m                           FutureWarning)\n\u001B[0;32m     71\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0minner_f\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36mconfusion_matrix\u001B[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    275\u001B[0m     \"\"\"\n\u001B[1;32m--> 276\u001B[1;33m     \u001B[0my_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_check_targets\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    277\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0my_type\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\"binary\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"multiclass\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    278\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"%s is not supported\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0my_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     80\u001B[0m     \"\"\"\n\u001B[0;32m     81\u001B[0m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 82\u001B[1;33m     \u001B[0mtype_true\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     83\u001B[0m     \u001B[0mtype_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001B[0m in \u001B[0;36mtype_of_target\u001B[1;34m(y)\u001B[0m\n\u001B[0;32m    248\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"y cannot be class 'SparseSeries' or 'SparseArray'\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 250\u001B[1;33m     \u001B[1;32mif\u001B[0m \u001B[0mis_multilabel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    251\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;34m'multilabel-indicator'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    252\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001B[0m in \u001B[0;36mis_multilabel\u001B[1;34m(y)\u001B[0m\n\u001B[0;32m    139\u001B[0m     \"\"\"\n\u001B[0;32m    140\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'__array__'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mSequence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m         \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"shape\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\ai\\lib\\site-packages\\numpy\\core\\_asarray.py\u001B[0m in \u001B[0;36masarray\u001B[1;34m(a, dtype, order)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     82\u001B[0m     \"\"\"\n\u001B[1;32m---> 83\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0morder\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\ai\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36m__array__\u001B[1;34m(self, dtype)\u001B[0m\n\u001B[0;32m    628\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__array__\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrelevant_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    629\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 630\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    631\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    632\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "print('\\nresult on train data')\n",
    "showMetrics(train_labels, train_predictions)\n",
    "print('\\nresult on test data')\n",
    "showMetrics(test_labels, test_predictions)\n",
    "\n",
    "cat_names = ['incorrect', 'correct']\n",
    "print('\\nresult on test data')\n",
    "showConfusionMatrix(train_labels, train_predictions, cat_names)\n",
    "print('\\nresult on test data')\n",
    "showConfusionMatrix(test_labels, test_predictions, cat_names)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cola.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}